# SAC Agent for FJSSP+PM

此目錄包含用於解決「彈性作業車間排程與預防性維護 (FJSSP+PM)」問題的 Soft Actor-Critic (SAC) Agent 實作。

## 核心觀念：為何在此問題上 SAC 優於 PPO？

實驗結果表明，SAC Agent 的表現遠超 PPO Agent。這主要源於兩者在強化學習演算法上的根本差異：

| 特性 | PPO (On-Policy) | SAC (Off-Policy) | 在此問題上的影響 |
| :--- | :--- | :--- | :--- |
| **數據利用效率** | **低**。像個「學完就忘」的學生，用完一批數據就丟棄。 | **高**。像個「反覆複習」的學霸，將所有經驗存入 Replay Buffer 中重複學習。 | 在複雜的排程問題中，每一次經驗都很寶貴。SAC 能更充分地從數據中學習，效率更高。 |
| **探索機制** | **隨機採樣**。策略可能過早收斂，陷入局部最優，不再探索新方法。 | **最大化熵 (Entropy)**。天生是個「好奇寶寶」，除了完成任務，也致力於讓動作盡可能隨機，探索更廣闊的空間。 | 排程問題的解空間是組合爆炸的。SAC 更強的探索能力讓它更有機會找到 PPO 發現不了的「神來之筆」。 |
| **學習穩定性** | **較差**。學習方向容易被單次「運氣不好」的經驗帶偏，導致訓練曲線劇烈震盪。 | **較好**。透過從 Replay Buffer 中隨機抽樣，平滑了單次經驗的影響。同時，其「雙 Critic」和「目標網路」的設計像穩定器，讓學習過程更平穩。 | 我們在 PPO 的訓練中看到了巨大的不穩定性，而 SAC 成功收斂，證明了其穩定性的優勢。 |

### On-Policy (PPO) vs. Off-Policy (SAC) 的核心差異：Replay Buffer

一個常見的問題是：為什麼我們需要 Replay Buffer？為什麼 PPO 沒有而 SAC 有？這正是兩類演算法的核心差異，也是 SAC 在此問題上表現更優越的關鍵。

#### 1. PPO (On-Policy): 「學完就忘」的學生

*   **原則**: PPO 的學習原則是「用**現在**的你，去評估你**剛才**的行為」。它假設產生數據的策略和正在學習的策略必須是同一個。
*   **問題**: 因此，PPO 每次用一批新數據更新完模型後，就必須**立刻把這批數據丟掉**，因為對於「更新後的你」來說，這些數據已經「過時」了。在我們這種模擬成本高昂的環境中，這極大地浪費了寶貴的數據。

#### 2. SAC (Off-Policy): 「反覆複習」的學霸

*   **原則**: SAC 的設計允許它區分「產生數據的策略」和「正在學習的策略」。
*   **優勢**: 這給了它巨大的靈活性，可以將成千上萬的舊經驗（無論好壞、來自哪個時期的策略）都存入 **Replay Buffer** 中。每次學習時，它都會從這個「經驗庫」中隨機抽取一批來「溫故知新」。

**結論**: 在我們這個環境中，不使用 Replay Buffer 的主要問題就是：**浪費數據導致學習緩慢，探索不足導致容易卡在不夠好的解決方案上**。SAC 憑藉 Replay Buffer 帶來的「高數據效率」和「更廣泛的經驗視野」，最終能找到更優的策略。

---

## 程式碼細節解析

### 1. 動作選擇機制 (Action Selection)

與單純使用 `softmax` 輸出機率不同，此處的 Agent 採用了更靈活的**機率分佈採樣**方法，以平衡「利用」與「探索」。

1.  **神經網路輸出 Logits**：Actor 網路的輸出是一組原始分數（logits），而非直接的機率。
2.  **建立機率分佈**：使用 `torch.distributions.Categorical(logits=...)` 將分數轉換為一個可操作的機率分佈物件。
3.  **從分佈中採樣**：呼叫 `.sample()` 方法從該分佈中隨機抽取一個動作。這確保了即使某個動作的機率不是最高，它仍有機會被選中去執行，這是**探索**的關鍵。

### 2. 如何處理不同尺寸的問題 (Padding)

為了讓固定大小的神經網路能處理不同機器數量的問題，程式碼採用了**填充 (Padding)** 技巧。

*   **狀態填充**：在 `train_sac.py` 中，我們設定了一個 `max_machines` 上限（例如 15）。在 `_encode_state` 函式中，無論當前問題有多少台機器，它產生的狀態向量永遠是為 15 台機器預留的長度。對於不存在的機器，其對應的特徵會用 **0** 來填充。
*   **動作填充**：同樣地，「維護」動作的輸出層長度也固定為 `max_machines`。如果 Agent 選擇了一個無效的動作（例如在 10 台機器的問題中選擇維護第 12 台），環境的 `step` 函式會自動忽略該動作。

### 3. 分層式決策架構 (Hierarchical Decision Architecture)

我們的 Agent 採用了「經理-專家」式的分層決策架構，將複雜問題拆解，其詳細互動流程如下：

1.  **經理 (上層 Agent) 進行戰略判斷**
    *   上層 Agent (`upper`) 接收最全面的**全域狀態** (`s_upper`)。
    *   它根據此狀態，做出宏觀的戰略決策：選擇「生產」(`action=0`) 還是「維護」(`action=1`)。

2.  **專家 (下層 Agent) 執行戰術**
    *   **如果指令是「生產」**：生產專家 (`prod`) 被激活。它接收與生產最相關的狀態 (`s_prod`)，並從 7 個生產排程規則中，挑選出當下最合適的一個。
    *   **如果指令是「維護」**：維護專家 (`pm`) 被激活。它接收與維護最相關的狀態 (`s_pm`)，並決定具體要對哪一台機器執行預防性維護。

3.  **分開學習與反思**
    *   在 `update` 學習階段，程式會確保「誰做的決策，誰來學習」。
    *   如果某次經驗的動作是「生產」，只有 `upper` 和 `prod` 兩個 Agent 的網路會被更新。
    *   反之，如果動作是「維護」，則只有 `upper` 和 `pm` 的網路會被更新。
    *   這個機制是透過 `update` 方法中的 `mask` 變數來實現的，它會過濾掉不相關的經驗，確保每個專家都能專注於自己的領域。

### 4. 並行處理的實現 (Implementation of Parallel Processing)

我們的模擬環境支持多台機器在同一時間段並行處理不同的工序。這個並行性是「湧現」出來的，其核心設計是為**每一台機器維護一個獨立的「空閒時間點」**。

*   **核心資料結構**: `FJSSP_PM_Env` 類別中的 `self.m_free` 字典。
    ```python
    self.m_free: Dict[int,int] = {k:0 for k in range(1,self.ins.machines+1)}
    ```
    這個字典的鍵是機器 ID，值是該機器將會得空的那個時間點。您可以把它想像成一個記錄著**多條獨立時間軸**的筆記本。

*   **實現邏輯**: 當一個工序被排到機器 `m` 上時：
    1.  程式會查詢 `self.m_free[m]` 來得知此機器的空閒時間。
    2.  計算出工序的開始與結束時間。
    3.  **最關鍵的一步**：程式**只會**更新 `self.m_free[m]` 的值，將該機器的時間軸向前推進。
    4.  所有其他機器的空閒時間點**保持不變**，因此它們可以隨時接收新的任務。

這種設計讓各機器的時間軸各自獨立向前延伸，從而在宏觀上形成了並行處理的效果。

### 5. 事件驅動與決策時間點 (Event-Driven Mechanism & Decision Time)

將「並行處理」與「事件驅動」的概念結合起來，可以更精準地理解模擬的運作方式。我們的模擬器本質上是一個**離散事件模擬 (Discrete Event Simulation)** 系統。

*   **什麼是「事件」**：在排程問題中，一個「事件」就是一個**需要 Agent 做出新決策的時間點**。主要事件包括：
    1.  機器完成一個工序。
    2.  機器完成一次維護。
    3.  一個新的作業到達系統 (僅限動態版本)。

*   **`self.m_free` 作為事件佇列**：`self.m_free` 字典扮演了「未來事件佇列」的核心角色。字典中的每一項 `m_free[k] = t_k` 都代表一個未來的事件：「在時間點 `t_k`，機器 `k` 將觸發一個『空閒』事件」。

*   **事件驅動的模擬迴圈**：
    1.  **找出下一個事件**：在任何時候，程式的第一步是找出所有未來事件中，**最早會發生的那一個**。這對應的程式碼就是 `min(self.m_free.values())`。
    2.  **時間跳躍**：模擬器的時鐘不會逐秒前進，而是直接**「跳躍」**到這個最早的事件發生時間點。
    3.  **觸發決策**：時間跳躍後，系統狀態發生改變（有機器變空閒了），這會觸發 Agent 進行一次新的決策。
    4.  **產生新事件**：當 Agent 做出決策（例如，安排一個新工序），它實際上是在**創造一個新的未來事件**，即更新對應機器的 `m_free` 值，將其設定為更晚的時間點。

這個「找出最早事件 -> 時間跳躍 -> 觸發決策 -> 產生新事件」的循環，就是整個模擬的核心驅動機制。

### 6. 冷啟動階段的決策機制 (t=0)

在模擬剛開始，所有機器都空閒時，系統有一個特殊的「冷啟動」行為。

*   **現象**：看起來像是在 `t=0` 時，連續派工了好幾個工序，把空閒的機器都填滿。
*   **本質**：這並不是一次決策派出多個工序。而是**在模擬時間 `t=0` 時，連續發生了多次決策**。
*   **流程拆解**：
    1.  **第一次決策 (t=0)**：`min(m_free)` 為 0。Agent 決策，將工序 A 派給機器 1。`m_free` 更新為 `{1: 10, 2: 0, 3: 0, ...}`。決策結束，狀態更新。
    2.  **第二次決策 (t=0)**：訓練迴圈繼續。程式再次計算 `min(m_free)`，發現最小值**仍然是 0**（來自機器 2 和 3）。因此，模擬時鐘**沒有前進**。Agent 觀察到新狀態，做出第二次決策，將工序 B 派給機器 2。`m_free` 更新為 `{1: 10, 2: 8, 3: 0, ...}`。
    3.  **重複此過程**：這個循環會一直持續，直到所有在 `t=0` 空閒的機器都被分配了任務。
    4.  **時間開始流動**：只有當所有機器的 `m_free` 值都大於 0 時，下一個決策時間點 `min(m_free)` 才會是一個大於 0 的數，模擬時鐘才會第一次真正地**向前跳躍**。

*   **規則重選**：在這個過程中，**每一次決策，Agent 都會重新觀察狀態並重新選擇派工規則**。它不是在 `t=0` 選定一個規則用到底，而是「用規則A派一個 -> 觀察 -> 用規則B派一個 -> 觀察...」，這給了 Agent 極大的靈活性來安排初始佈局。

---

## 如何解讀訓練結果

### 「改善率 (Improvement Rate)」指標

這是評估 Agent 性能最公平的指標。

*   **公式**：`(Agent 成本 - 基準成本) / 基準成本`
*   **基準**：一個固定的「最短加工時間優先 (SPT)」策略。
*   **解讀**：
    *   **負數（越低越好）**：代表 Agent 的表現優於簡單的 SPT 策略。例如，`-0.86` 意味著成本比基準低 86%。
    *   **正數**：代表 Agent 的表現還不如簡單的 SPT 策略。
*   **目標**：我們希望看到「改善率」的移動平均線能穩定地收斂到一個盡可能低的負值。

---
## 核心數學模型 (Core Mathematical Models)

本專案的目標是最小化一個綜合的總成本函數，其背後由幾個關鍵的數學模型支撐。

### 1. 總成本目標函數 (The Objective Function)

系統的總目標是最小化總成本 `Total Cost`，它由三個部分組成：
`Total Cost = TTC (延遲成本) + TBC (負載平衡成本) + TMC (維護成本)`

#### 總延遲成本 (Total Tardiness Cost, TTC)
*   **模型**: 對於每一個「完成時間」超過「交付日期」的作業，計算其延遲時間，並乘上該作業的懲罰權重。
    > `TTC = Σ [ max(0, 完成時間ᵢ - 交付日期ᵢ) × 延遲懲罰ᵢ ]`
*   **Code**: 由 `calc_TTC()` 函數實現，遍歷所有作業計算延遲懲罰。

#### 總平衡成本 (Total Balancing Cost, TBC)
*   **模型**: 懲罰「機器之間工作負載不均」的情況。它計算所有機器總加工時間的**標準差**，並乘上一個係數 `Cb`。負載越不均衡，此成本越高。
    > `TBC = Cb × √[ Var(各機器的總加工時間) ]`
*   **Code**: 由 `calc_TBC()` 函數實現。
*   **參數 `Cb`**: 此係數的預設值為 **500**。但在訓練時，會從 **[400, 600]** 區間內隨機選取，以增強模型的泛化能力。

#### 總維護成本 (Total Maintenance Cost, TMC)
*   **模型**: 估算排程週期內的預期維護總成本，包含預防性維護 `Cp` 和潛在的故障風險成本。
    > `TMCₖ = Cp + Cf × ∫λ(t)dt` (單台機器 k 的成本)
*   **Code**: 由 `_estimate_TMC()` 方法實現，它會對每台機器計算此預期成本並加總。

### 2. 預測性維護模型 (Predictive Maintenance Model)

這是整個系統最核心的數學模型，描述了機器的健康狀態如何隨時間衰退。

#### 基礎故障率：韋伯分佈 (Weibull Distribution)
*   **模型**: 使用**韋伯分佈**來模擬機器的瞬時故障率 `λ(t)`，這是可靠度工程中的經典模型。`t` 是機器的「年齡」（距離上次保養後運作的時間）。
    > `λ(t) = (θ/η) × (t/η)^(θ-1)`
*   **Code**: 由 `weibull_lambda(t, theta, eta)` 函數實現。

#### 考慮作業特性的故障率 (PHM-informed Failure Rate)
*   **模型**: 假設「不同的作業對機器的損耗不同」。此影響透過一個「PHM 乘子」實現，該乘子會放大或縮小基礎故障率。
    > `λ_actual(t) = λ_weibull(t) × Multiplier`
    > `Multiplier = exp(βᵀX)`
    > * `X`: 作業的物理特性向量（轉速、扭矩等）。
    * `β`: 機器對這些物理特性的敏感度係數向量。
*   **Code**: 由 `_current_cycle_multiplier(k)` 方法實現，計算機器 `k` 在當前維護週期內的平均損耗加速因子。

#### 最佳維護時機 (T\*)
*   **模型**: `T*` 是一個參考指標，代表在當前狀態下，理論上再運作多久後進行保養，長期來看「單位時間的總成本」最低。
*   **Code**: 由 `_find_T_star_numeric(...)` 方法實現。由於公式複雜，程式採用**數值搜尋**：嘗試一系列的 `T` 值，找出哪個 `T` 能讓成本函數 `VC(T)` 最小。這個計算出的 `T*` 會作為一個重要的特徵提供給 Agent。

程式碼透過一個兩階段的**數值搜尋演算法**來求解 `T*`，其詳細步驟如下：

```pseudocode
Algorithm: Find_Optimal_T_Numeric
Input: 維護參數 (Cp, Cf, Tp, Tf, θ, η), 當前損耗乘數 (m)
Output: 最佳維護間隔 T*

1.  // 定義成本函數 VC(T)
2.  function VC(T):
3.      I ← m * (T/η)^θ      // 計算預期故障數
4.      TotalCost ← Cp + Cf * I
5.      TotalTime ← T + Tp + Tf * I
6.      return TotalCost / TotalTime

7.  // === 第一步：粗略搜尋 ===
8.  min_cost ← infinity
9.  T_coarse ← Tp // 預設一個初始值
10. for T in range(1, 600, step=5): // 以 5 為步長，在一個大範圍內搜尋
11.     cost ← VC(T)
12.     if cost < min_cost:
13.         min_cost ← cost
14.         T_coarse ← T

15. // === 第二步：精細搜尋 ===
16. T_star ← T_coarse
17. // 在粗略找到的最佳點 T_coarse 周圍，進行更密集的搜尋
18. for T_fine in a dense set of points around T_coarse (e.g., 81 points in [T_coarse-20, T_coarse+20]):
19.     cost ← VC(T_fine)
20.     if cost < min_cost:
21.         min_cost ← cost
22.         T_star ← T_fine

23. return T_star
```

### 3. 關鍵參數 β (beta) 的設定

`β` 向量用來模擬不同作業對機器損耗程度的差異。

1.  **在訓練過程中 (隨機生成)**:
    *   使用 `InstanceGenerator` 時，每個作業的 `β` 向量是從**常態分佈**（平均值 1.0，標準差 0.3）中隨機抽樣生成的 4 個數字。這讓 Agent 能學習應對不同損耗特性的工作。
    *   **Code**:
        ```python
        # In InstanceGenerator.sample_instance:
        beta = [float(self.rng.normal(1.0, 0.3)) for _ in range(4)]
        ```

2.  **在固定的測試案例中**:
    *   使用 `build_full_instance_from_table_A2` 等函數時，`β` 是直接寫在程式碼裡的**固定值**，以確保測試比較的公平性。
    *   **Code**:
        ```python
        # In build_full_instance_from_table_A2:
        jobs.append(Job(1, ..., [1.095, 0.479, 0.134, 0.754]))
        ```

---
## 如何使用

1.  **訓練 SAC Agent**:
    ```bash
    python baseline/sac/train_sac.py --train_dist
    ```
    *   日誌會儲存在 `baseline/sac/train_log_sac.csv`。
    *   最佳模型會儲存在 `checkpoints/sac_best.pt`。

2.  **繪製學習曲線**:
    *   訓練結束後，執行以下指令：
    ```bash
    python baseline/sac/plot_sac_logs.py
    ```
    *   這會在 `baseline/sac/` 資料夾中產生三張圖，其中 `sac_improvement_rate_curve.png` 最具參考價值。
---
## 參數設定 (Parameter Configuration)

### 1. 訓練腳本參數 (Training Script Arguments)

這些參數可以透過命令列在執行 `train_sac.py` 時進行調整。

| 參數 | 預設值 | 說明 |
| :--- | :--- | :--- |
| `--episodes` | 8000 | 訓練的總回合數。 |
| `--steps_per_ep` | 500 | 每個回合的最大決策步數。 |
| `--max_machines` | 15 | 模擬環境中支援的最大機器數量。 |
| `--seed` | 0 | 用於重現性的隨機種子。 |
| `--train_dist` | (flag) | **(必要)** 使用隨機實例生成器進行訓練。 |
| `--demo` | (flag) | 使用固定的 `demo` 實例進行偵錯。 |
| `--baseline_rule`| 2 | 用於比較基準的派工規則 ID (預設 2=SPT)。 |

*(註：對於 `sac_dynamic` 版本，還有 `--horizon` 參數用於設定模擬時間上限)*

### 2. 強化學習超參數 (Reinforcement Learning Hyperparameters)

這些參數同樣在 `train_sac.py` 中定義，主要影響 SAC 演算法的學習行為。

| 參數 | 預設值 | 說明 |
| :--- | :--- | :--- |
| `--lr_actor` | 3e-5 | Actor (演員) 網路的學習率。 |
| `--lr_critic` | 3e-4 | Critic (評論家) 網路的學習率。 |
| `--gamma` | 0.99 | 獎勵的折扣因子，決定 Agent 的「遠見」程度。 |
| `--tau` | 0.005 | Target 網路軟更新的係數，τ 越小更新越平滑。 |
| `--alpha` | 0.2 | SAC 的熵溫度係數，控制探索的強度。 |
| `--capacity` | 100000 | Replay Buffer (經驗回放池) 的容量。 |
| `--batch_size` | 256 | 每次從 Replay Buffer 中抽樣學習的經驗數量。 |
| `--hidden_dim` | 256 | 神經網路隱藏層的維度（神經元數量）。 |
| `--start_steps` | 2500 | 在正式開始訓練前，完全隨機探索的步數。 |

### 3. 環境與問題定義參數

這些參數大多在 `core_sac.py` 中定義，描述了問題本身的物理和經濟屬性。

*   **成本相關參數**:
    *   `lambda_tbc`: (在 `train_sac.py` 中) TBC 成本的權重，預設為 `1.0`。
    *   `Cb`: (在 `Instance` 中) TBC 成本的係數。預設為 `500`，但在訓練時會在 `[400, 600]` 區間隨機選取。
    *   `tardy_penalty`: (在 `Job` 中) 作業的單位時間延遲懲罰。在訓練時會在 `[300, 600]` 區間隨機選取。

*   **維護模型參數 (來自 `TABLE_A1`)**:
    *   `theta` (θ): 韋伯分佈的**形狀參數**，影響故障率曲線的形狀。
    *   `eta` (η): 韋伯分佈的**尺度參數**，影響設備的特徵壽命。
    *   `Tp`: 預防性維護 (PM) 所需的固定**時間**。
    *   `Cp`: 執行一次預防性維護的**成本**。
    *   `Tf`: 發生故障後，修復所需的**時間**。
    *   `Cf`: 發生故障後的修復**成本**（通常遠高於 `Cp`）。

*   **實例生成器參數 (硬編碼在 `InstanceGenerator` 中)**:
    *   `N` (作業數量): 在 `[8, 17]` 之間隨機選取。
    *   `L` (單一作業的工序數): 在 `[3, 6]` 之間隨機選取。
    *   `pt` (工序加工時間): 在 `[6, 25]` 之間隨機選取。
    *   `arrival_rate` (λ): (僅動態版) 在 `[0.01, 0.08]` 之間隨機選取。
